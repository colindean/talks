## Titles?

* Meaningful AI: Mass Enablement for Better Humanity
* AI Is Built On Human Behavior: the Good and the Bad
* Crafting Meaningful AI Tools: AI = Humans + Math
* Generate Time Savers, Not Life Wreckers
* Use AI to Cure Cancer, not to Make People Click Ads [^jerome]

[^jerome]: Homage to Jerome Pesenti.

## General thoughts

LLMs are good at text generation, brainstorming ideas, and simple analysis.

Millennials will assuredly remember being advised by high school and college teachers never to trust Wikipedia,
and always search for other sources.
That is, it was OK to start there but never rely solely on it.

_Never rely solely on an LLM._
Always check else, and don't expect the LLM to give you its sources.
_Retrieval Augmented Generation_ is a developing field that combines traditional search with generative AI.

<!-- 
  @misc{ enwiki:1241186708,
    author = "{Wikipedia contributors}",
    title = "Stochastic parrot --- {Wikipedia}{,} The Free Encyclopedia",
    year = "2024",
    url = "https://en.wikipedia.org/w/index.php?title=Stochastic_parrot&oldid=1241186708",
    note = "[Online; accessed 30-August-2024]"
  }
-->

> The term **stochastic parrot** is a metaphor to describe the theory that large language models, though able to generate plausible language, do not understand the meaning of the language they process.

**Stochastic Parrot** coined in _On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?_ by Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Margaret Mitchell.

**Hallucination**: LLMs will occasionally synthesize information that matches some pattern, but not reality. (from the same paper!)

> A computer can never be held accountable, therefore a computer must never make a management decision"
>
> — IBM 

## Good projects

### [Fight Health Insurance](https://fighthealthinsurance.com/)

Using LLMs to write insurance denial appeals.

<img width="1334" alt="Screenshot of Fight Health Insurance, 2024-08-30" src="https://github.com/user-attachments/assets/01887b3b-b558-49fd-970a-e1e5ec6faf11">

### Legal analysis

https://technical.ly/software-development/legal-ai-tools-pittsburgh-law-firm-artifex/

#### Coverage 

* [‘Make your health insurance company cry’: One woman’s fight to turn the tables on insurers ](https://sfstandard.com/2024/08/23/holden-karau-fight-health-insurance-appeal-claims-denials/)

## Good posts and podcasts

### AI is more than GenAI - Practical AI

https://pca.st/episode/97efa108-d43b-4d6a-b8ac-064792ab3c43

this episode was a great overview of the modern eras of AI
in about the last 14 years, and covers the advancements in
natural language processing, computer vision and such
before getting into high level overview of what generative AI does
then explores some of the subfields within generative AI.

### Existential Risks - Dana Fried

> Just a reminder that the "existential risk" from AI is not
> that somehow we'll make Skynet or the computers from The Matrix.
>
> Nobody is going to give a large language model the nuclear codes.
>
> The existential risk is to marginalized people who will be silently refused jobs or health care or parole,
> or who will be targeted by law enforcement or military action because of an ML model's inherent bias,
> and that because these models are black boxes, it will be nearly impossible for victims to appeal.
>
> The existential risk is that the incredible repository of nearly all human knowledge
> that is the internet will be flooded with so much LLM-generated dreck that locating reliable information
> will become effectively impossible (alongside scientific journals,
> which are also suffering incredibly under the weight of ML spam).
>
> The existential risk is that nobody will be able to trust a photo or video of anything
> because the vast majority of media will be fabricated.
>
> The existential risk posed by AI is that we as a species will no longer be able
> to transmit and build on generational knowledge,
> which is the primary thing that has allowed human society to advance since the end of the last ice age.

– Dana Fried (@tess)
[2024-08-29 18:27 PM ET](https://mastodon.social/@tess/113047665053440692)
[2nd in series](https://mastodon.social/@tess/113047677790105168),
[3rd in series](https://mastodon.social/@tess/113047693906163761)

### Model autophagy 

https://mastodon.social/@jensorensen/113136950728170129

## Scary things

https://blog.apaonline.org/2023/04/13/responsibility-and-automated-decision-making-draft/

https://gizmodo.com/googles-ai-will-help-decide-whether-unemployed-workers-get-benefits-2000496215

https://www.semafor.com/article/09/17/2024/black-teenagers-twice-as-likely-to-be-falsely-accused-of-using-ai-tools-in-homework

https://www.threads.net/@katienotopoulos/post/DAGbXlKunUr

https://www.facebook.com/blackforager/videos/1187609425902231/

https://assemblag.es/@inquiline/113183425618440170

## Fluff

https://hachyderm.io/@molly0xfff/113155131527029562

https://fs.blog/douglas-adams-reactions-technology-over-time/
